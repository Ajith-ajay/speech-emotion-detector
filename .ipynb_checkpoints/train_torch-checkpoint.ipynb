{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79a0cd-e7db-4260-b521-e71b4138355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa soundfile numpy scikit-learn pyaudio torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9e10c7-fd44-4cb6-82d7-6cbf3e0df0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import soundfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5801639-5bcd-4ff4-8787-a1e1080ff82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc=True, chroma=True, mel=True):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        result = np.array([])\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e30633-9d77-4567-8717-b18202fe7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01':'neutral', '02':'calm', '03':'happy', '04':'sad',\n",
    "    '05':'angry', '06':'fearful', '07':'disgust', '08':'surprised'\n",
    "}\n",
    "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511a0ce5-75f3-4afd-a8e0-274d9a6e8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.25):\n",
    "    x, y = [], []\n",
    "    for file in glob.glob(\"D:\\\\Projects\\\\Machine Learning\\\\speech emotion detector\\\\data\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        features = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(features)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee4d692-7844-49ee-b4b5-ec7d3eb3aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "786dd801-d401-42f9-b73a-97a39fda6854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 180), (192, 180))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f7da978-3ba9-4693-b9e8-6ec5bc73a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515f1c8f-7be8-4c8f-acda-b9f0669189da",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af6f857-c61a-4cf7-9ea3-5625ccbc48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedd854e-4ce7-4135-b91b-a84da0b29acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(x_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0173fe-36e7-4266-968e-fd2a21db8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(EmotionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167998aa-a028-4da9-b045-21dee2091887",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73204885-3b49-43c1-aeb4-24e8786c73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionNet(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79fa9757-20c9-4772-8b3e-54d99fdf2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 1.3020\n",
      "Epoch 2/50 - Loss: 1.1379\n",
      "Epoch 3/50 - Loss: 0.9890\n",
      "Epoch 4/50 - Loss: 0.8709\n",
      "Epoch 5/50 - Loss: 0.7711\n",
      "Epoch 6/50 - Loss: 0.7272\n",
      "Epoch 7/50 - Loss: 0.6166\n",
      "Epoch 8/50 - Loss: 0.5478\n",
      "Epoch 9/50 - Loss: 0.4926\n",
      "Epoch 10/50 - Loss: 0.4377\n",
      "Epoch 11/50 - Loss: 0.4060\n",
      "Epoch 12/50 - Loss: 0.3802\n",
      "Epoch 13/50 - Loss: 0.3723\n",
      "Epoch 14/50 - Loss: 0.3096\n",
      "Epoch 15/50 - Loss: 0.2924\n",
      "Epoch 16/50 - Loss: 0.2647\n",
      "Epoch 17/50 - Loss: 0.2225\n",
      "Epoch 18/50 - Loss: 0.2077\n",
      "Epoch 19/50 - Loss: 0.2264\n",
      "Epoch 20/50 - Loss: 0.2074\n",
      "Epoch 21/50 - Loss: 0.2047\n",
      "Epoch 22/50 - Loss: 0.1946\n",
      "Epoch 23/50 - Loss: 0.1981\n",
      "Epoch 24/50 - Loss: 0.1572\n",
      "Epoch 25/50 - Loss: 0.1390\n",
      "Epoch 26/50 - Loss: 0.1626\n",
      "Epoch 27/50 - Loss: 0.1222\n",
      "Epoch 28/50 - Loss: 0.1266\n",
      "Epoch 29/50 - Loss: 0.1429\n",
      "Epoch 30/50 - Loss: 0.1383\n",
      "Epoch 31/50 - Loss: 0.1836\n",
      "Epoch 32/50 - Loss: 0.1425\n",
      "Epoch 33/50 - Loss: 0.1211\n",
      "Epoch 34/50 - Loss: 0.1260\n",
      "Epoch 35/50 - Loss: 0.1039\n",
      "Epoch 36/50 - Loss: 0.0892\n",
      "Epoch 37/50 - Loss: 0.0784\n",
      "Epoch 38/50 - Loss: 0.0703\n",
      "Epoch 39/50 - Loss: 0.0616\n",
      "Epoch 40/50 - Loss: 0.0536\n",
      "Epoch 41/50 - Loss: 0.0746\n",
      "Epoch 42/50 - Loss: 0.0549\n",
      "Epoch 43/50 - Loss: 0.0618\n",
      "Epoch 44/50 - Loss: 0.0653\n",
      "Epoch 45/50 - Loss: 0.0519\n",
      "Epoch 46/50 - Loss: 0.0716\n",
      "Epoch 47/50 - Loss: 0.0488\n",
      "Epoch 48/50 - Loss: 0.0493\n",
      "Epoch 49/50 - Loss: 0.0571\n",
      "Epoch 50/50 - Loss: 0.0567\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/50 - Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b51fd3-da61-49f4-ae9d-ce3df38c395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.52%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e39ad72-878a-4ffc-b657-c586b10a13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fearful\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(file_path):\n",
    "    features = extract_feature(file_path, mfcc=True, chroma=True, mel=True)\n",
    "    features = scaler.transform([features])\n",
    "    tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        prediction = torch.argmax(output).item()\n",
    "        return le.inverse_transform([prediction])[0]\n",
    "\n",
    "# Example usage\n",
    "print(predict_emotion(\"D:\\\\Projects\\\\Machine Learning\\\\speech emotion detector\\\\test\\\\hap.wav\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gender Classification",
   "language": "python",
   "name": "genderclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
